{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeelayS/ppe-detection/blob/main/processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW8xlOf2sckM",
        "outputId": "f8667e2e-e6f2-44c8-8977-293062197626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/projects/ppe\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/projects/ppe/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "iW-rXoYAYcFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, io\n",
        "from os.path import join\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "mbjV4MLr4_S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "rx7aV7ZvYg1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PictorPPEDataset(Dataset):\n",
        "    def __init__(self, img_dir, annotations_list):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "\n",
        "        with open(annotations_list, \"rb\") as f:\n",
        "            self.annotations = sorted(f.readlines())\n",
        "        f.close()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        annotation = str(self.annotations[idx])[2:-5].split(\"\\\\t\")\n",
        "\n",
        "        img_name = annotation[0]\n",
        "        img = io.read_image(join(self.img_dir, img_name))\n",
        "\n",
        "        cropped_detections = []\n",
        "        labels = []\n",
        "\n",
        "        for det_annotation in annotation[1:]:\n",
        "\n",
        "            label = int(det_annotation[-1])\n",
        "            if label==2 or label==3:\n",
        "              label=1\n",
        "            labels.append(label)\n",
        "\n",
        "            x1, y1, x2, y2 = list(map(lambda x: int(x), det_annotation[:-2].split(\",\")))\n",
        "            cropped_detection = img[:, y1:y2, x1:x2].float()\n",
        "            cropped_detections.append(cropped_detection)\n",
        "\n",
        "        return cropped_detections, labels\n"
      ],
      "metadata": {
        "id": "oYH7JSfr5Fyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PictorPPEDataset(\"./Images\", \"Labels/pictor_ppe_crowdsourced_approach-02_train.txt\")"
      ],
      "metadata": {
        "id": "JKsf3Gbx9WId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "lgb39SV_9jMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = next(iter(dataloader))\n",
        "print(type(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSxxCqiAfNdU",
        "outputId": "43ff64dc-4a02-48aa-ccb6-39a5e9dea079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l_0 = 0\n",
        "l_1 = 0\n",
        "l_2 = 0\n",
        "l_3 = 0\n",
        "\n",
        "for _, labels in dataloader:\n",
        "  for label in labels:\n",
        "    if label.item()==0:\n",
        "      l_0 += 1\n",
        "    elif label.item()==1:\n",
        "      l_1 += 1\n",
        "    elif label.item()==2:\n",
        "      l_2 += 1\n",
        "    else:\n",
        "      l_3 += 1\n",
        "    \n",
        "print(l_0, l_1, l_2, l_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOiAfNxzSoZc",
        "outputId": "04e6ee56-94ce-44d2-eea3-3c3aa8ed19e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "577 1069 0 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample[1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Hnr6qW69_FJ",
        "outputId": "2e681dc5-3aa7-4754-8e2c-773f502e3763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Labels/pictor_ppe_crowdsourced_approach-02_train.txt\", 'rb') as f:\n",
        "  annotations = sorted(f.readlines())"
      ],
      "metadata": {
        "id": "frbnVUJs6E3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(annotations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZiQUVtr6QUz",
        "outputId": "6d7917e6-55c7-460d-e60a-26e565ef116a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str(annotations[1])[2:-5].split(\"\\\\t\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwY0_wUA6R13",
        "outputId": "2a953daf-913b-400e-ef2e-a9dc05477d80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['image_from_china(10).jpg',\n",
              " '143,165,202,277,0',\n",
              " '319,157,328,192,0',\n",
              " '343,158,355,188,0',\n",
              " '328,157,339,191,0']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str(annotations[1])[2:-5].split(\"\\\\t\")[1][:-2].split(\",\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_B7ILQ4o6Tvv",
        "outputId": "fc9f3c92-290d-4077-b780-69e68012451b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['143', '165', '202', '277']"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crop = sample[0][0]\n",
        "crop.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGrC7wS_aHVI",
        "outputId": "37b2aa05-cdf6-4062-c78e-7fb096cb0486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 145, 48])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "nb2C6cPBaNP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop = crop.squeeze(0).permute(1, 2, 0).numpy()\n",
        "plt.imshow(crop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "Uo9gjvSbaRrB",
        "outputId": "7e817984-2ae4-422c-dcb2-19b3d89ef9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1e61105c50>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition"
      ],
      "metadata": {
        "id": "4Tj6izByb5bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorchyolo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lbzXVTFEcA6Q",
        "outputId": "7cb1c2bb-5473-448b-ecb2-b79591e26c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorchyolo\n",
            "  Downloading PyTorchYolo-1.6.2-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 829 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from pytorchyolo) (2.8.0)\n",
            "Collecting imgaug<0.5.0,>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 8.4 MB/s \n",
            "\u001b[?25hCollecting terminaltables<4.0.0,>=3.1.0\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /usr/local/lib/python3.7/dist-packages (from pytorchyolo) (4.63.0)\n",
            "Collecting Pillow<9.0.0,>=8.1.0\n",
            "  Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 28.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from pytorchyolo) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from pytorchyolo) (1.10.0+cu111)\n",
            "Collecting matplotlib<4.0.0,>=3.3.3\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 30.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from pytorchyolo) (1.21.5)\n",
            "Requirement already satisfied: torchsummary<2.0.0,>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from pytorchyolo) (1.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.5.0,>=0.4.0->pytorchyolo) (1.15.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug<0.5.0,>=0.4.0->pytorchyolo) (1.8.1.post1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.5.0,>=0.4.0->pytorchyolo) (0.18.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug<0.5.0,>=0.4.0->pytorchyolo) (4.1.2.30)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug<0.5.0,>=0.4.0->pytorchyolo) (2.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug<0.5.0,>=0.4.0->pytorchyolo) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.3.3->pytorchyolo) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.3.3->pytorchyolo) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.3.3->pytorchyolo) (0.11.0)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.31.2-py3-none-any.whl (899 kB)\n",
            "\u001b[K     |████████████████████████████████| 899 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.3.3->pytorchyolo) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0.0,>=3.3.3->pytorchyolo) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4.0.0,>=3.3.3->pytorchyolo) (3.10.0.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug<0.5.0,>=0.4.0->pytorchyolo) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug<0.5.0,>=0.4.0->pytorchyolo) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug<0.5.0,>=0.4.0->pytorchyolo) (1.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (3.17.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (2.23.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (1.44.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3.0.0,>=2.4.0->pytorchyolo) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3.0.0,>=2.4.0->pytorchyolo) (3.2.0)\n",
            "Installing collected packages: Pillow, fonttools, matplotlib, terminaltables, imgaug, pytorchyolo\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-8.4.0 fonttools-4.31.2 imgaug-0.4.0 matplotlib-3.5.1 pytorchyolo-1.6.2 terminaltables-3.1.10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from pytorchyolo import detect, models as yolo_models\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import io, models\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, features_dim, layers_config=[1024, 512]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features_dim = features_dim\n",
        "\n",
        "        layers_config = [features_dim] + list(layers_config)\n",
        "\n",
        "        self.head = nn.ModuleList()\n",
        "        for i in range(0, len(layers_config) - 1):\n",
        "            self.head.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Linear(layers_config[i], layers_config[i + 1]),\n",
        "                    nn.ReLU(),\n",
        "                ),\n",
        "            )\n",
        "        self.head.append(nn.Linear(layers_config[-1], 2))\n",
        "        self.head = nn.Sequential(*self.head)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "class YoloV3DetectionModel:\n",
        "    def __init__(self, config_path, weights_path, threshold=0.5):\n",
        "\n",
        "        self.threshold = threshold\n",
        "\n",
        "        self.model = yolo_models.load_model(config_path, weights_path)\n",
        "\n",
        "    def _filter_detections(self, detections):\n",
        "\n",
        "        filtered_detections = []\n",
        "\n",
        "        for detection in detections:\n",
        "\n",
        "            if (\n",
        "                detection[-2] > self.threshold and int(detection[-1]) == 0\n",
        "            ):  # 0 = person class\n",
        "                filtered_detections.append(list(map(lambda x: int(x), detection[:-2])))\n",
        "\n",
        "        return filtered_detections\n",
        "\n",
        "    def __call__(self, img_path):\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        detections = detect.detect_image(self.model, img)\n",
        "        detections = self._filter_detections(detections)\n",
        "\n",
        "        print(f\"Detected {len(detections)} persons\")\n",
        "\n",
        "        img = io.read_image(img_path)\n",
        "\n",
        "        cropped_detections = []\n",
        "        for detection in detections:\n",
        "            cropped_detections.append(\n",
        "                img[:, detection[1] : detection[3], detection[0] : detection[2]]\n",
        "                .unsqueeze(0)\n",
        "                .float()\n",
        "            )\n",
        "\n",
        "        return cropped_detections\n",
        "\n",
        "\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(\n",
        "        self, reshape_size, features_dim, layers_config, n_heads, backbone=None\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.reshape_size = reshape_size\n",
        "\n",
        "        if backbone is None:\n",
        "            self.backbone = models.vgg19(pretrained=True).features\n",
        "        else:\n",
        "            self.backbone = backbone\n",
        "\n",
        "        self.heads = nn.ModuleList()\n",
        "        for _ in range(n_heads):\n",
        "            self.heads.append(\n",
        "                ClassificationHead(\n",
        "                    features_dim=features_dim, layers_config=layers_config\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, detections):\n",
        "\n",
        "        outs = []\n",
        "\n",
        "        for detection in detections:\n",
        "\n",
        "            detection = F.interpolate(\n",
        "                detection, self.reshape_size, mode=\"bilinear\", align_corners=True\n",
        "            )\n",
        "            features = self.backbone(detection)\n",
        "\n",
        "            instance_outs = []\n",
        "\n",
        "            for head in self.heads:\n",
        "                out = head(features)\n",
        "                instance_outs.append(out)\n",
        "\n",
        "            outs.append(instance_outs)\n",
        "\n",
        "        return outs\n",
        "\n",
        "\n",
        "class CompleteModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        detection_threshold,\n",
        "        detection_config,\n",
        "        detection_weights,\n",
        "        detection_reshape_size,\n",
        "        classification_features_dim,\n",
        "        classification_layers_config,\n",
        "        classification_n_heads,\n",
        "        classification_model_weights=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.detection_model = YoloV3DetectionModel(\n",
        "            config_path=detection_config,\n",
        "            weights_path=detection_weights,\n",
        "            threshold=detection_threshold,\n",
        "        )\n",
        "        self.classification_model = ClassificationModel(\n",
        "            reshape_size=detection_reshape_size,\n",
        "            features_dim=classification_features_dim,\n",
        "            layers_config=classification_layers_config,\n",
        "            n_heads=classification_n_heads,\n",
        "        )\n",
        "\n",
        "        if classification_model_weights is not None:\n",
        "            self.classification_model.load_state_dict(classification_model_weights)\n",
        "\n",
        "    def forward(self, img_path):\n",
        "\n",
        "        detections = self.detection_model(img_path)\n",
        "        img_outs = self.classification_model(detections)\n",
        "\n",
        "        return img_outs\n",
        "\n",
        "\n",
        "# Map outs for a detection uniquely to the detection\n",
        "# Pre-process images\n",
        "# Detection model\n",
        "# Load pre-trained weights"
      ],
      "metadata": {
        "id": "oehk8ZyKb9yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClassificationModel((96, 32), 1536, (512, 128), 1)"
      ],
      "metadata": {
        "id": "tXDn_MFac4K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model(sample[0])\n",
        "type(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWOUmpnMebqu",
        "outputId": "19311202-3413-4ccb-b221-4959e55e60eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = [o[0] for o in out]\n",
        "out = torch.cat(out)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et13qrN3fg-A",
        "outputId": "7cbe591e-0c11-4a08-a4e9-547d1a12d2b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out, sample[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t1Nr9dkgpNe",
        "outputId": "8d1f0f6e-ad06-4418-ad55-762f6d991e94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1794, -0.3224],\n",
              "         [ 0.1898, -0.2405],\n",
              "         [ 0.1642, -0.1922]], grad_fn=<CatBackward0>),\n",
              " [tensor([1]), tensor([1]), tensor([1])])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = torch.LongTensor(sample[1])\n",
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVemQDCRg8sA",
        "outputId": "fee42d80-30bc-4e0e-effb-b52a142aed8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn(out, sample[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h9zwIj2hopc",
        "outputId": "2e660d27-86e3-42ee-b4b9-0bd73f4f96f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9312, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "JoOtFRPmYjOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(preds, labels):\n",
        "\n",
        "  preds = [pred[0] for pred in preds]\n",
        "  preds = torch.cat(preds)\n",
        "  # labels = torch.LongTensor(labels)\n",
        "\n",
        "  loss = F.cross_entropy(preds, labels)\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "kxUk2BsxYnXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(model, dataloader, optimizer, epochs, device, save_dir):\n",
        "\n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "\n",
        "  iter_losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for i, (data, labels) in enumerate(dataloader):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      data = [d.to(device) for d in data]\n",
        "      labels = torch.LongTensor(labels).to(device)\n",
        "\n",
        "     # data, labels = data.to(device), labels.to(device)\n",
        "      preds = model(data)\n",
        "\n",
        "      loss = loss_fn(preds, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (i+1)%200==0:\n",
        "        print(f\"Iteration {(epoch*len(dataloader)) + i}: Loss = {loss.item()}\")\n",
        "        iter_losses.append(loss.item())\n",
        "\n",
        "      if i%5000==0:\n",
        "        torch.save(model.state_dict(), join(save_dir, \"iter\" + str((epoch*len(dataloader)) + i) + \"_model.pth\"))\n",
        "\n",
        "  plt.plot(iter_losses)"
      ],
      "metadata": {
        "id": "YMzY_KH96bPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PictorPPEDataset(\"data/Images\", \"data/Labels/pictor_ppe_crowdsourced_approach-02_train.txt\")\n",
        "trainloader = DataLoader(train_dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "ggvEiW-DmJYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = ClassificationModel((96, 32), 1536, (512, 128), 1)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJGfR5RTk8tf",
        "outputId": "975dfed1-428a-43bc-ad87-5721e6065dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "jbbp3vgCk_Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "train_classifier(model, trainloader, optimizer, epochs, device, \"model_weights/pretrained\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "p9Z_1EFllBgV",
        "outputId": "5d81479c-7c6b-434f-bce3-a40fbf6bfa93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 199: Loss = 0.7168130278587341\n",
            "Iteration 399: Loss = 0.5851792693138123\n",
            "Iteration 701: Loss = 0.7531083226203918\n",
            "Iteration 901: Loss = 0.6314525008201599\n",
            "Iteration 1203: Loss = 0.7489344477653503\n",
            "Iteration 1403: Loss = 0.5780285000801086\n",
            "Iteration 1705: Loss = 0.7614888548851013\n",
            "Iteration 1905: Loss = 0.6236931085586548\n",
            "Iteration 2207: Loss = 0.7548441290855408\n",
            "Iteration 2407: Loss = 0.5963820815086365\n",
            "Iteration 2709: Loss = 0.7581562995910645\n",
            "Iteration 2909: Loss = 0.5867300033569336\n",
            "Iteration 3211: Loss = 0.7525709271430969\n",
            "Iteration 3411: Loss = 0.5757452845573425\n",
            "Iteration 3713: Loss = 0.6598060727119446\n",
            "Iteration 3913: Loss = 0.5690398216247559\n",
            "Iteration 4215: Loss = 0.022593608126044273\n",
            "Iteration 4415: Loss = 0.5575740337371826\n",
            "Iteration 4717: Loss = 0.18011169135570526\n",
            "Iteration 4917: Loss = 0.5598032474517822\n",
            "Iteration 5219: Loss = 0.21478445827960968\n",
            "Iteration 5419: Loss = 0.47238364815711975\n",
            "Iteration 5721: Loss = 0.02652120031416416\n",
            "Iteration 5921: Loss = 0.4299851655960083\n",
            "Iteration 6223: Loss = 0.1392771601676941\n",
            "Iteration 6423: Loss = 0.7858474850654602\n",
            "Iteration 6725: Loss = 0.1664404720067978\n",
            "Iteration 6925: Loss = 0.5200234651565552\n",
            "Iteration 7227: Loss = 0.06493737548589706\n",
            "Iteration 7427: Loss = 0.30327415466308594\n",
            "Iteration 7729: Loss = 0.034429240971803665\n",
            "Iteration 7929: Loss = 0.3254951536655426\n",
            "Iteration 8231: Loss = 0.0611611008644104\n",
            "Iteration 8431: Loss = 0.9525273442268372\n",
            "Iteration 8733: Loss = 0.7206384539604187\n",
            "Iteration 8933: Loss = 0.6161705851554871\n",
            "Iteration 9235: Loss = 0.7600307464599609\n",
            "Iteration 9435: Loss = 0.600620687007904\n",
            "Iteration 9737: Loss = 0.7632598280906677\n",
            "Iteration 9937: Loss = 0.5994606018066406\n",
            "Iteration 10239: Loss = 0.763687789440155\n",
            "Iteration 10439: Loss = 0.5986944437026978\n",
            "Iteration 10741: Loss = 0.7643022537231445\n",
            "Iteration 10941: Loss = 0.5961857438087463\n",
            "Iteration 11243: Loss = 0.5449294447898865\n",
            "Iteration 11443: Loss = 0.5828675627708435\n",
            "Iteration 11745: Loss = 0.7673769593238831\n",
            "Iteration 11945: Loss = 0.5981781482696533\n",
            "Iteration 12247: Loss = 0.7643772959709167\n",
            "Iteration 12447: Loss = 0.5950348973274231\n",
            "Iteration 12749: Loss = 0.7646464705467224\n",
            "Iteration 12949: Loss = 0.5985522270202637\n",
            "Iteration 13251: Loss = 0.7650019526481628\n",
            "Iteration 13451: Loss = 0.5911365151405334\n",
            "Iteration 13753: Loss = 0.37988099455833435\n",
            "Iteration 13953: Loss = 0.4850254952907562\n",
            "Iteration 14255: Loss = 0.47832635045051575\n",
            "Iteration 14455: Loss = 0.7388718128204346\n",
            "Iteration 14757: Loss = 0.3670888841152191\n",
            "Iteration 14957: Loss = 0.25934842228889465\n",
            "Iteration 15259: Loss = 0.08234596997499466\n",
            "Iteration 15459: Loss = 0.45097577571868896\n",
            "Iteration 15761: Loss = 0.1941201537847519\n",
            "Iteration 15961: Loss = 0.5675013065338135\n",
            "Iteration 16263: Loss = 0.029823003336787224\n",
            "Iteration 16463: Loss = 0.6708338856697083\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-89d1441e01af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_weights/pretrained\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-eda59be8eba5>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(model, dataloader, optimizer, epochs, device, save_dir)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-eda59be8eba5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "noBmLUH0zu0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(model, testloader, device):\n",
        "\n",
        "  n_total = 0\n",
        "  n_correct = 0\n",
        "\n",
        "  p = []\n",
        "  l = []\n",
        "\n",
        "  for data, labels in testloader:\n",
        "\n",
        "    data = [d.to(device) for d in data]\n",
        "    labels = torch.LongTensor(labels).to(device)\n",
        "\n",
        "    preds = model(data)\n",
        "    preds = [pred[0] for pred in preds]\n",
        "    preds = torch.cat(preds)\n",
        "    preds = F.softmax(preds, dim=1)\n",
        "    preds = torch.argmax(preds, dim=1)\n",
        "\n",
        "    p += list(preds)\n",
        "    l += list(labels)\n",
        "\n",
        "    n_correct += torch.sum(torch.eq(preds, labels))\n",
        "    n_total += len(labels)\n",
        "\n",
        "  accuracy = (n_correct/n_total)*100\n",
        "  print(f\"Accuracy: {accuracy}%\")\n",
        "\n",
        "  p = [i.cpu().item() for i in p]\n",
        "  l = [i.cpu().item() for i in l]\n",
        "\n",
        "  return accuracy.item(), p, l"
      ],
      "metadata": {
        "id": "PUZssVnRROky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = PictorPPEDataset(\"data/Images\", \"data/Labels/pictor_ppe_crowdsourced_approach-02_test.txt\")\n",
        "testloader = DataLoader(test_dataset, batch_size=1)\n",
        "len(test_dataset), len(testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdQ0uqfbzuXl",
        "outputId": "aca60c08-3ae8-485f-ea3d-e2afaa61ebf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(152, 152)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = ClassificationModel((96, 32), 1536, (512, 128), 1)\n",
        "model.load_state_dict(torch.load(\"model_weights/scratch/best_model.pth\", map_location=\"cpu\"))\n",
        "model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toO0Ug5fz04f",
        "outputId": "8ed8fd0f-82f7-4f03-b474-43f34ebe3e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc, preds, labels = eval(model, testloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7mobVGj1xFN",
        "outputId": "dc8e8f17-f741-4be3-d52b-9b71f1f0e129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.25345611572266%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix"
      ],
      "metadata": {
        "id": "22FqEG2sU2m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(preds, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ObOkwXEVU_6",
        "outputId": "c424f097-66d4-4d82-c93a-191d5485f076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8525345622119815"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(preds, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS4XbCf1VX2U",
        "outputId": "dd5ab614-a79a-451e-aab6-2c58439f401d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[112,  26],\n",
              "       [ 38, 258]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    }
  ]
}